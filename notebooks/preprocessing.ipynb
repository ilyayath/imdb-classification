{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1Gj3lgIkfn861ZbcQUAdRTVnRgdqgzXiZ",
      "authorship_tag": "ABX9TyOph1msnCzNFrw2bzbHMBTi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ilyayath/imdb-classification/blob/main/preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "from datasets import load_dataset\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from lxml.html import fromstring\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vw32W7L39xe9",
        "outputId": "04e743e8-ca5c-4169-eabb-ce598ab2f5c2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"imdb\")\n",
        "\n",
        "df_train = dataset['train'].to_pandas()\n",
        "df_test = dataset['test'].to_pandas()"
      ],
      "metadata": {
        "id": "VbvnchHsQrYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger_eng')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vyx6sTP5HuwS",
        "outputId": "b8bec2c1-e954-4605-da9f-3f56a736aee6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "au5TXjOuBLjb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_wordnet_pos(tag):\n",
        "    if tag.startswith('J'):\n",
        "        return 'a'\n",
        "    elif tag.startswith('V'):\n",
        "        return 'v'\n",
        "    elif tag.startswith('N'):\n",
        "        return 'n'\n",
        "    elif tag.startswith('R'):\n",
        "        return 'r'\n",
        "    else:\n",
        "        return 'n'"
      ],
      "metadata": {
        "id": "JQ8Gz0mqOFWC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing(text):\n",
        "  try:\n",
        "    tree = fromstring(text)\n",
        "    pure = tree.text_content()\n",
        "  except Exception:\n",
        "    pure = text\n",
        "\n",
        "  pure = pure.lower()\n",
        "  tokens = word_tokenize(pure)\n",
        "  tagged_tokens = pos_tag(tokens)\n",
        "  lemmatized_sentence = []\n",
        "\n",
        "  for word, tag in tagged_tokens:\n",
        "    if word == 'are' or word in ['is', 'am']:\n",
        "        lemmatized_sentence.append(word)\n",
        "    else:\n",
        "        lemmatized_sentence.append(lemmatizer.lemmatize(word, get_wordnet_pos(tag)))\n",
        "  flt_tokens = [\n",
        "      token for token in lemmatized_sentence\n",
        "      if token not in stop_words\n",
        "      and token not in string.punctuation\n",
        "      and token.isalpha()\n",
        "  ]\n",
        "  return flt_tokens"
      ],
      "metadata": {
        "id": "ZJyB68haERMn"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train['text'] = df_train['text'].apply(preprocessing)\n",
        "df_test['text'] = df_test['text'].apply(preprocessing)"
      ],
      "metadata": {
        "id": "VDs1mN_iD3BR"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.to_csv('train_upd.csv')\n",
        "df_test.to_csv('test_upd.csv')"
      ],
      "metadata": {
        "id": "eOQBY49FGz5H"
      },
      "execution_count": 12,
      "outputs": []
    }
  ]
}
